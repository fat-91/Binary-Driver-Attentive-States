{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fat-91/Binary-Driver-Attentive-States/blob/main/ERP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3pR4oKrP-Vbo"
      },
      "outputs": [],
      "source": [
        "!pip install mne numpy pandas scikit-learn tensorflow matplotlib seaborn\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZtowj2xA9h8"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, MaxPooling1D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "DEVIATION_MARKER_IDS = {'dev_left': 1, 'dev_right': 2}\n",
        "RESPONSE_MARKER_ID = {'response': 3}\n",
        "TMIN, TMAX = -0.2, 0.8\n",
        "channels = ['FZ', 'CZ', 'PZ', 'P3', 'P4']\n",
        "RT_THRESHOLD_PERCENTILE = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z952T5HPGlpG"
      },
      "outputs": [],
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XEgDfa9zfTd"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/My Drive/data'\n",
        "\n",
        "full_paths = [os.path.join(base_path, fname) for fname in file_list]\n",
        "raws = []\n",
        "for fpath in full_paths:\n",
        "    # Read the EEG file (EEGLAB .set format)\n",
        "    raw = mne.io.read_raw_eeglab(fpath, preload=True)\n",
        "    common = [ch for ch in channels if ch in raw.ch_names]\n",
        "    raw.pick(common)\n",
        "    raws.append(raw)\n",
        "\n",
        "raw = mne.concatenate_raws(raws)\n",
        "print(\"Combined data shape (channels, timepoints):\", raw.get_data().shape)\n",
        "\n",
        "#Extracting the raw data as a NumPy array.\n",
        "data = raw.get_data()\n",
        "sfreq = raw.info['sfreq']\n",
        "events, event_id = mne.events_from_annotations(raw)\n",
        "print(sfreq)\n",
        "\n",
        "channel_map = {}\n",
        "for fpath in full_paths:\n",
        "    raw_temp = mne.io.read_raw_eeglab(fpath, preload=False)\n",
        "    channel_map[fpath] = raw_temp.ch_names\n",
        "\n",
        "for fname, chs in channel_map.items():\n",
        "    print(f\"{fname} â†’ {chs}\")\n",
        "\n",
        "\n",
        "selected_channels = raw.ch_names\n",
        "print(\"\\nSelected Channels after filtering:\", selected_channels)\n",
        "\n",
        "print(\"\\nSnippet of Extracted Data (first 5 time points for each channel):\")\n",
        "for i, ch_name in enumerate(selected_channels):\n",
        "    print(f\"{ch_name}: {data[i, :5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UiL6ewakLBX"
      },
      "outputs": [],
      "source": [
        "def get_reaction_times(events, sfreq):\n",
        "    deviation_indices = [i for i, event_id in enumerate(events[:, 2]) if event_id in DEVIATION_MARKER_IDS.values()]\n",
        "    rt_data = []\n",
        "    for dev_idx in deviation_indices:\n",
        "        dev_time = events[dev_idx, 0]\n",
        "        next_responses = [j for j in range(dev_idx + 1, len(events)) if events[j, 2] in RESPONSE_MARKER_ID.values()]\n",
        "        if next_responses:\n",
        "            rt = (events[next_responses[0], 0] - dev_time) / sfreq\n",
        "            rt_data.append({'event_index': dev_idx, 'rt': rt})\n",
        "        else:\n",
        "            rt_data.append({'event_index': dev_idx, 'rt': np.nan})\n",
        "    return pd.DataFrame(rt_data)\n",
        "\n",
        "def create_labels(rt_df, threshold_percentile):\n",
        "    valid_rts = rt_df['rt'].dropna()\n",
        "    if valid_rts.empty:\n",
        "        rt_df['label'] = np.nan\n",
        "        return rt_df, np.nan\n",
        "    threshold = np.percentile(valid_rts, threshold_percentile)\n",
        "    rt_df['label'] = rt_df['rt'].apply(lambda x: 1 if x > threshold else 0)\n",
        "    return rt_df, threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqHrFVxNkRVz"
      },
      "outputs": [],
      "source": [
        "rts = get_reaction_times(events, sfreq)\n",
        "rts = rts.dropna(subset=['rt'])\n",
        "rts, rt_thresh = create_labels(rts, RT_THRESHOLD_PERCENTILE)\n",
        "\n",
        "# --- Create Labeled Events and Metadata ---\n",
        "valid_event_indices = rts['event_index'].values\n",
        "labeled_events = events[valid_event_indices]\n",
        "metadata = pd.DataFrame({'label': rts['label'].values})\n",
        "\n",
        "# --- Epoching ---\n",
        "epochs = mne.Epochs(\n",
        "    raw, labeled_events,\n",
        "    event_id={str(v): v for v in DEVIATION_MARKER_IDS.values()},\n",
        "    tmin=TMIN, tmax=TMAX, metadata=metadata,\n",
        "    baseline=None, preload=True)\n",
        "\n",
        "# --- Final Dataset ---\n",
        "X = epochs.get_data()\n",
        "y = epochs.metadata['label'].values.astype(int)\n",
        "X = np.transpose(X, (0, 2, 1))\n",
        "\n",
        "\n",
        "# --- Optimization Strategies ---\n",
        "# 1. Data Augmentation (Simple Time Warping - careful with EEG)\n",
        "def time_warp(data, sigma=0.1):\n",
        "    from scipy.interpolate import CubicSpline\n",
        "    n_samples, n_times, n_channels = data.shape\n",
        "    new_data = np.zeros_like(data)\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_channels):\n",
        "            tt = np.arange(n_times)\n",
        "            warp_factor = np.random.normal(loc=1.0, scale=sigma, size=n_times)\n",
        "            tt_stretched = np.cumsum(warp_factor)\n",
        "            tt_stretched /= np.max(tt_stretched)\n",
        "            f = CubicSpline(tt_stretched, data[i, :, j])\n",
        "            tt_new = np.linspace(0, 1, n_times)\n",
        "            new_data[i, :, j] = f(tt_new)\n",
        "    return new_data\n",
        "\n",
        "# Consider applying augmentation only to the training set\n",
        "X_augmented = time_warp(X)\n",
        "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
        "    X_augmented, y, test_size=0.3, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqNFf5y9T1xd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(f\"Epochs object created with {len(epochs)} epochs.\")\n",
        "    print(f\"Metadata labels distribution:\\n{epochs.metadata['label'].value_counts()}\")\n",
        "    print(f\"Available channels in epochs: {epochs.ch_names}\")\n",
        "except NameError:\n",
        "    print(\"Error: The 'epochs' object does not seem to exist.\")\n",
        "    print(\"Please make sure you have successfully run the previous code sections.\")\n",
        "    # Exit or raise error if epochs aren't ready\n",
        "    exit()\n",
        "\n",
        "available_channels = epochs.ch_names\n",
        "picks_n200 = [ch for ch in ['FZ', 'CZ'] if ch in available_channels]\n",
        "picks_p300 = [ch for ch in ['PZ', 'P3', 'P4'] if ch in available_channels]\n",
        "theta_channel_names = ['FZ', 'CZ'] # Check spelling/case!\n",
        "picks_theta = [ch for ch in theta_channel_names if ch in available_channels]\n",
        "\n",
        "print(f\"\\nUsing channels for N200: {picks_n200}\")\n",
        "print(f\"Using channels for P300: {picks_p300}\")\n",
        "print(f\"Using channels for Theta: {picks_theta}\")\n",
        "\n",
        "# Check if any pick lists are empty\n",
        "if not picks_n200:\n",
        "    print(\"Warning: No suitable channels found for N200 analysis.\")\n",
        "if not picks_p300:\n",
        "    print(\"Warning: No suitable channels found for P300 analysis.\")\n",
        "if not picks_theta:\n",
        "    print(\"Warning: No suitable channels found for Theta analysis.\")\n",
        "\n",
        "baseline_period = (TMIN, 0) # (-0.2, 0) seconds\n",
        "print(f\"Applying baseline correction: {baseline_period}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZNfp3zq5kFu"
      },
      "outputs": [],
      "source": [
        "# Separate epochs based on the 'label' in metadata\n",
        "epochs_fast = epochs[epochs.metadata['label'] == 0]\n",
        "epochs_slow = epochs[epochs.metadata['label'] == 1]\n",
        "\n",
        "print(f\"\\nNumber of fast RT epochs (label 0): {len(epochs_fast)}\")\n",
        "print(f\"Number of slow RT epochs (label 1): {len(epochs_slow)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHQ-nCWhFVzS"
      },
      "outputs": [],
      "source": [
        "print(\"Epochs data shape:\", epochs.get_data().shape)\n",
        "print(\"Channels used:\", epochs.info['ch_names'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNEVYWHtrXJz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"NaNs in X_train_aug:\", np.isnan(X_train_aug).sum())\n",
        "print(\"Infs in X_train_aug:\", np.isinf(X_train_aug).sum())\n",
        "print(\"NaNs in y_train_aug:\", np.isnan(y_train_aug).sum())\n",
        "print(\"Infs in y_train_aug:\", np.isinf(y_train_aug).sum())\n",
        "\n",
        "np.unique(y_train_aug, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb7md8l-r3ax"
      },
      "outputs": [],
      "source": [
        "print(\"y_train labels:\", np.unique(y_train_aug, return_counts=True))\n",
        "print(\"y_test labels:\", np.unique(y_test_aug, return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7hXAkF90KE44"
      },
      "outputs": [],
      "source": [
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "all_histories = []\n",
        "all_val_aucs = []\n",
        "best_model = None\n",
        "best_auc = -1\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Normalize per fold (using overall mean/std from training set)\n",
        "    X_mean_fold = X_train.mean()\n",
        "    X_std_fold = X_train.std()\n",
        "    X_train_norm_fold = (X_train - X_mean_fold) / X_std_fold\n",
        "    X_val_norm_fold   = (X_val   - X_mean_fold) / X_std_fold\n",
        "\n",
        "    # Calculate class weights per fold\n",
        "    class_weights_fold = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weights_dict_fold = dict(enumerate(class_weights_fold))\n",
        "    print(\"Class Weights (Fold {}):\".format(fold + 1), class_weights_dict_fold)\n",
        "    model = Sequential([\n",
        "        # Input layer with shape as per fold\n",
        "        Input(shape=(X_train_norm_fold.shape[1], X_train_norm_fold.shape[2])),\n",
        "\n",
        "        # First convolution block\n",
        "        Conv1D(32, kernel_size=7, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Second convolution block\n",
        "        Conv1D(64, kernel_size=5, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Third convolution block - optional if your data supports extra complexity\n",
        "        Conv1D(128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with an Adam optimizer and a learning rate you can experiment with.\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "    # Callbacks: early stopping on val_auc, model checkpoint saving best model per fold, and reduce LR on plateau.\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=20, restore_best_weights=True, monitor='val_auc', mode='max'),\n",
        "        ModelCheckpoint(f\"best_model_fold_{fold + 1}.keras\", monitor='val_auc', mode='max', save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, verbose=1, mode='max', min_lr=1e-5)\n",
        "    ]\n",
        "\n",
        "    # Train the model for this fold\n",
        "    history = model.fit(\n",
        "        X_train_norm_fold, y_train,\n",
        "        validation_data=(X_val_norm_fold, y_val),\n",
        "        epochs=100,  # You may increase epochs; early stopping is active.\n",
        "        batch_size=32,\n",
        "        class_weight=class_weights_dict_fold,\n",
        "        callbacks=callbacks,\n",
        "        verbose=2\n",
        "    )\n",
        "    all_histories.append(history.history)\n",
        "\n",
        "    # Load the best model from this fold based on highest val_auc\n",
        "    loaded_model_fold = tf.keras.models.load_model(f\"best_model_fold_{fold + 1}.keras\")\n",
        "\n",
        "    # Evaluate on the validation set of this fold\n",
        "    _, _, val_auc = loaded_model_fold.evaluate(X_val_norm_fold, y_val, verbose=0)\n",
        "    print(f\"Fold {fold + 1} Validation AUC: {val_auc:.4f}\")\n",
        "    all_val_aucs.append(val_auc)\n",
        "\n",
        "    if val_auc > best_auc:\n",
        "        best_auc = val_auc\n",
        "        best_model = loaded_model_fold\n",
        "\n",
        "\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_mean_full = X_train_full.mean()\n",
        "X_std_full = X_train_full.std()\n",
        "X_test_norm_full = (X_test_full - X_mean_full) / X_std_full\n",
        "\n",
        "loss, acc, auc = best_model.evaluate(X_test_norm_full, y_test_full, verbose=0)\n",
        "print(\"\\n--- Best Model Evaluation on Test Set ---\")\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(f\"Test AUC: {auc:.4f}\")\n",
        "\n",
        "# Generate predictions and evaluation metrics\n",
        "y_pred_prob = best_model.predict(X_test_norm_full)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test_full, y_pred)\n",
        "cm = confusion_matrix(y_test_full, y_pred)\n",
        "report = classification_report(y_test_full, y_pred)\n",
        "\n",
        "print(f\"\\nF1 Score: {f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j2s2DwRuX5Rh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- General Plot for Loss Across All Folds ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i, history in enumerate(all_histories):\n",
        "    fold_num = i + 1\n",
        "    epochs = range(1, len(history['loss']) + 1)\n",
        "    plt.plot(epochs, history['loss'], label=f'Fold {fold_num} - Train Loss', linestyle='--')\n",
        "    plt.plot(epochs, history['val_loss'], label=f'Fold {fold_num} - Validation Loss', linestyle='-')\n",
        "\n",
        "plt.title('Loss Across All Folds')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- General Plot for Accuracy Across All Folds ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i, history in enumerate(all_histories):\n",
        "    fold_num = i + 1\n",
        "    epochs = range(1, len(history['accuracy']) + 1)\n",
        "    plt.plot(epochs, history['accuracy'], label=f'Fold {fold_num} - Train Accuracy', linestyle='--')\n",
        "    plt.plot(epochs, history['val_accuracy'], label=f'Fold {fold_num} - Validation Accuracy', linestyle='-')\n",
        "\n",
        "plt.title('Accuracy Across All Folds')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- General Plot for AUC Across All Folds ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i, history in enumerate(all_histories):\n",
        "    fold_num = i + 1\n",
        "    epochs = range(1, len(history['auc']) + 1)\n",
        "    plt.plot(epochs, history['auc'], label=f'Fold {fold_num} - Train AUC', linestyle='--')\n",
        "    plt.plot(epochs, history['val_auc'], label=f'Fold {fold_num} - Validation AUC', linestyle='-')\n",
        "\n",
        "plt.title('AUC Across All Folds')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Visualize Validation AUCs Across Folds\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(range(1, n_splits + 1), all_val_aucs, color='skyblue')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Validation AUC')\n",
        "plt.title('Validation AUC for Each Fold')\n",
        "plt.xticks(range(1, n_splits + 1))\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# 3. Visualize Confusion Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative (0)', 'Positive (1)'],\n",
        "            yticklabels=['Negative (0)', 'Positive (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix on Test Set')\n",
        "plt.show()\n",
        "\n",
        "# 4. Visualize ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test_full, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuEjLzQ1C_RH"
      },
      "outputs": [],
      "source": [
        "# --- General Plot for Loss Across All Folds (Averaged) ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "all_train_loss = [history['loss'] for history in all_histories]\n",
        "all_val_loss = [history['val_loss'] for history in all_histories]\n",
        "\n",
        "min_epochs_loss = min(len(loss) for loss in all_train_loss)\n",
        "avg_train_loss = np.mean([loss[:min_epochs_loss] for loss in all_train_loss], axis=0)\n",
        "avg_val_loss = np.mean([val_loss[:min_epochs_loss] for val_loss in all_val_loss], axis=0)\n",
        "\n",
        "epochs_loss = range(1, min_epochs_loss + 1)\n",
        "plt.plot(epochs_loss, avg_train_loss, 'r--', label='Average Train Loss')\n",
        "plt.plot(epochs_loss, avg_val_loss, 'b-', label='Average Validation Loss')\n",
        "plt.title('Average Loss Across All Folds')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- General Plot for Accuracy Across All Folds (Averaged) ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "all_train_accuracy = [history['accuracy'] for history in all_histories]\n",
        "all_val_accuracy = [history['val_accuracy'] for history in all_histories]\n",
        "\n",
        "min_epochs_accuracy = min(len(acc) for acc in all_train_accuracy)\n",
        "avg_train_accuracy = np.mean([acc[:min_epochs_accuracy] for acc in all_train_accuracy], axis=0)\n",
        "avg_val_accuracy = np.mean([val_acc[:min_epochs_accuracy] for val_acc in all_val_accuracy], axis=0)\n",
        "\n",
        "epochs_accuracy = range(1, min_epochs_accuracy + 1)\n",
        "plt.plot(epochs_accuracy, avg_train_accuracy, 'r--', label='Average Train Accuracy')\n",
        "plt.plot(epochs_accuracy, avg_val_accuracy, 'b-', label='Average Validation Accuracy')\n",
        "plt.title('Average Accuracy Across All Folds')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- General Plot for AUC Across All Folds (Averaged) ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "all_train_auc = [history['auc'] for history in all_histories]\n",
        "all_val_auc = [history['val_auc'] for history in all_histories]\n",
        "\n",
        "min_epochs_auc = min(len(auc) for auc in all_train_auc)\n",
        "avg_train_auc = np.mean([auc[:min_epochs_auc] for auc in all_train_auc], axis=0)\n",
        "avg_val_auc = np.mean([val_auc[:min_epochs_auc] for val_auc in all_val_auc], axis=0)\n",
        "\n",
        "epochs_auc = range(1, min_epochs_auc + 1)\n",
        "plt.plot(epochs_auc, avg_train_auc, 'r--', label='Average Train AUC')\n",
        "plt.plot(epochs_auc, avg_val_auc, 'b-', label='Average Validation AUC')\n",
        "plt.title('Average AUC Across All Folds')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ3n6d8Yjdsf"
      },
      "outputs": [],
      "source": [
        "test_events = np.zeros((len(y_pred), 3), dtype=int)\n",
        "test_events[:, 0] = np.arange(len(y_pred))  # Dummy sample numbers\n",
        "test_events[:, 2] = 1  # Using a dummy event ID\n",
        "\n",
        "test_epochs_data = np.transpose(X_test_full, (0, 2, 1)) # Ensure correct shape (n_epochs, n_channels, n_times)\n",
        "\n",
        "info = mne.create_info(ch_names=channels, sfreq=sfreq, ch_types='eeg')\n",
        "test_epochs = mne.EpochsArray(test_epochs_data, info, events=test_events, tmin=TMIN)\n",
        "\n",
        "# Add the model's predictions as metadata\n",
        "predicted_labels = pd.DataFrame({'label': y_pred.flatten()})\n",
        "test_epochs.metadata = predicted_labels\n",
        "\n",
        "# Separate epochs based on the model's predicted labels\n",
        "epochs_fast_pred = test_epochs[test_epochs.metadata['label'] == 0]\n",
        "epochs_slow_pred = test_epochs[test_epochs.metadata['label'] == 1]\n",
        "\n",
        "print(f\"\\nNumber of epochs predicted as fast RT (label 0): {len(epochs_fast_pred)}\")\n",
        "print(f\"Number of epochs predicted as slow RT (label 1): {len(epochs_slow_pred)}\")\n",
        "\n",
        "baseline_period = (TMIN, 0)\n",
        "\n",
        "if len(epochs_fast_pred) > 0:\n",
        "    evoked_fast_pred = epochs_fast_pred.average().apply_baseline(baseline=baseline_period)\n",
        "else:\n",
        "    evoked_fast_pred = None\n",
        "    print(\"Warning: No epochs predicted as fast RT.\")\n",
        "\n",
        "if len(epochs_slow_pred) > 0:\n",
        "    evoked_slow_pred = epochs_slow_pred.average().apply_baseline(baseline=baseline_period)\n",
        "else:\n",
        "    evoked_slow_pred = None\n",
        "    print(\"Warning: No epochs predicted as slow RT.\")\n",
        "\n",
        "n200_window = (0.15, 0.28)\n",
        "p300_window = (0.28, 0.60)\n",
        "theta_window = (0.10, 0.60)\n",
        "theta_freqs = np.arange(4, 9, 1)\n",
        "n_cycles_theta = theta_freqs / 2.0\n",
        "picks_n200 = [ch for ch in ['FZ', 'CZ'] if ch in channels]\n",
        "picks_p300 = [ch for ch in ['PZ', 'P3', 'P4'] if ch in channels]\n",
        "picks_theta = [ch for ch in ['FZ', 'CZ'] if ch in channels]\n",
        "\n",
        "# --- N200 Analysis based on Model Predictions ---\n",
        "print(\"\\n--- N200 Analysis (Based on Model Predictions) ---\")\n",
        "if picks_n200 and evoked_fast_pred is not None and evoked_slow_pred is not None:\n",
        "    fig_n200_pred = mne.viz.plot_compare_evokeds(\n",
        "        {'Predicted Fast RT (label 0)': evoked_fast_pred, 'Predicted Slow RT (label 1)': evoked_slow_pred},\n",
        "        picks=picks_n200,\n",
        "        title=f'N200 Comparison (Predicted Labels - {\", \".join(picks_n200)})',\n",
        "        show_sensors='upper right',\n",
        "        legend='upper left',\n",
        "        ci=0.95\n",
        "    )\n",
        "    current_fig_n200_pred = fig_n200_pred[0] if isinstance(fig_n200_pred, list) else fig_n200_pred\n",
        "    for i, ax in enumerate(current_fig_n200_pred.axes[:len(picks_n200)]):\n",
        "        ax.axvspan(n200_window[0], n200_window[1], color='gray', alpha=0.2, label='N200 Window')\n",
        "        # Add vertical lines for potential peaks (you might need to calculate these as before)\n",
        "    plt.show()\n",
        "elif not picks_n200:\n",
        "    print(\"Skipping N200 analysis: No suitable channels found.\")\n",
        "else:\n",
        "    print(\"Skipping N200 analysis: Not enough predicted epochs for both conditions.\")\n",
        "\n",
        "# --- P300 Analysis based on Model Predictions ---\n",
        "print(\"\\n--- P300 Analysis (Based on Model Predictions) ---\")\n",
        "if picks_p300 and evoked_fast_pred is not None and evoked_slow_pred is not None:\n",
        "    fig_p300_pred = mne.viz.plot_compare_evokeds(\n",
        "        {'Predicted Fast RT (label 0)': evoked_fast_pred, 'Predicted Slow RT (label 1)': evoked_slow_pred},\n",
        "        picks=picks_p300,\n",
        "        title=f'P300 Comparison (Predicted Labels - {\", \".join(picks_p300)})',\n",
        "        show_sensors='upper right',\n",
        "        legend='upper left',\n",
        "        ci=0.95\n",
        "    )\n",
        "    current_fig_p300_pred = fig_p300_pred[0] if isinstance(fig_p300_pred, list) else fig_p300_pred\n",
        "    for i, ax in enumerate(current_fig_p300_pred.axes[:len(picks_p300)]):\n",
        "        ax.axvspan(p300_window[0], p300_window[1], color='gray', alpha=0.2, label='P300 Window')\n",
        "        # Add vertical lines for potential peaks\n",
        "    plt.show()\n",
        "elif not picks_p300:\n",
        "    print(\"Skipping P300 analysis: No suitable channels found.\")\n",
        "else:\n",
        "    print(\"Skipping P300 analysis: Not enough predicted epochs for both conditions.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"picks_theta: {picks_theta}\")\n",
        "print(f\"Number of epochs predicted as fast RT (label 0): {len(epochs_fast_pred)}\")\n",
        "print(f\"Number of epochs predicted as slow RT (label 1): {len(epochs_slow_pred)}\")\n",
        "print(\"\\n--- Time-Frequency Analysis for Theta (Based on Model Predictions) ---\")\n",
        "\n",
        "# Ensure n_cycles_theta is compatible with epoch length and freqs\n",
        "# Example: n_cycles_theta = theta_freqs / 2.0\n",
        "\n",
        "if picks_theta and len(epochs_fast_pred) > 0 and len(epochs_slow_pred) > 0:\n",
        "    try:\n",
        "        # Use the newer .compute_tfr method\n",
        "        tfr_fast_pred = epochs_fast_pred.compute_tfr(\n",
        "            method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "            return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "        )\n",
        "        tfr_slow_pred = epochs_slow_pred.compute_tfr(\n",
        "            method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "            return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "        )\n",
        "\n",
        "        # Apply baseline correction (percentage change is common for power)\n",
        "        tfr_fast_pred.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "        tfr_slow_pred.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "\n",
        "        # Calculate the averaged TFR for plotting TFR maps\n",
        "        avg_tfr_fast_pred = tfr_fast_pred.average()\n",
        "        avg_tfr_slow_pred = tfr_slow_pred.average()\n",
        "\n",
        "\n",
        "\n",
        "        # --- Visualize Average Theta Power Over Time ---\n",
        "        times_tfr_pred = avg_tfr_fast_pred.times  # Get time points from the averaged TFR\n",
        "        theta_power_fast_pred = avg_tfr_fast_pred.data.mean(axis=(0, 1))  # Mean over channels & freqs\n",
        "        theta_power_slow_pred = avg_tfr_slow_pred.data.mean(axis=(0, 1))\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times_tfr_pred, theta_power_fast_pred, label='Predicted Fast Condition')\n",
        "        plt.plot(times_tfr_pred, theta_power_slow_pred, label='Predicted Slow Condition')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Average Theta Power (% Change from Baseline)')\n",
        "        plt.title('Average Theta Power Over Time (Based on Model Predictions)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during TFR analysis (predicted labels): {e}\")\n",
        "elif not picks_theta:\n",
        "    print(\"Skipping Theta TFR analysis: No suitable channels found.\")\n",
        "else:\n",
        "    print(\"Skipping Theta TFR analysis: Not enough predicted epochs for both conditions.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx7Xvzk7p32W"
      },
      "outputs": [],
      "source": [
        "# Normalize training data\n",
        "X_train_norm_full = (X_train_full - X_mean_full) / X_std_full\n",
        "\n",
        "# Predict on training data\n",
        "y_pred_prob_train = best_model.predict(X_train_norm_full)\n",
        "y_pred_train = (y_pred_prob_train > 0.5).astype(int)\n",
        "\n",
        "# Create dummy events for training data\n",
        "train_events = np.zeros((len(y_pred_train), 3), dtype=int)\n",
        "train_events[:, 0] = np.arange(len(y_pred_train))\n",
        "train_events[:, 2] = 1  # Dummy event code\n",
        "\n",
        "# Prepare EEG data: (n_epochs, n_channels, n_times)\n",
        "train_epochs_data = np.transpose(X_train_full, (0, 2, 1))\n",
        "\n",
        "# Create MNE Epochs\n",
        "info = mne.create_info(ch_names=channels, sfreq=sfreq, ch_types='eeg')\n",
        "train_epochs = mne.EpochsArray(train_epochs_data, info, events=train_events, tmin=TMIN)\n",
        "\n",
        "# Add predictions as metadata\n",
        "train_epochs.metadata = pd.DataFrame({'label': y_pred_train.flatten()})\n",
        "\n",
        "# Split into predicted classes\n",
        "epochs_fast_pred = train_epochs[train_epochs.metadata['label'] == 0]\n",
        "epochs_slow_pred = train_epochs[train_epochs.metadata['label'] == 1]\n",
        "\n",
        "\n",
        "print(f\"\\nNumber of epochs predicted as fast RT (label 0): {len(epochs_fast_pred)}\")\n",
        "print(f\"Number of epochs predicted as slow RT (label 1): {len(epochs_slow_pred)}\")\n",
        "\n",
        "baseline_period = (TMIN, 0)\n",
        "\n",
        "if len(epochs_fast_pred) > 0:\n",
        "    evoked_fast_pred = epochs_fast_pred.average().apply_baseline(baseline=baseline_period)\n",
        "else:\n",
        "    evoked_fast_pred = None\n",
        "    print(\"Warning: No epochs predicted as fast RT.\")\n",
        "\n",
        "if len(epochs_slow_pred) > 0:\n",
        "    evoked_slow_pred = epochs_slow_pred.average().apply_baseline(baseline=baseline_period)\n",
        "else:\n",
        "    evoked_slow_pred = None\n",
        "    print(\"Warning: No epochs predicted as slow RT.\")\n",
        "\n",
        "n200_window = (0.15, 0.28)\n",
        "p300_window = (0.28, 0.60)\n",
        "theta_window = (0.10, 0.60)\n",
        "theta_freqs = np.arange(4, 9, 1)\n",
        "n_cycles_theta = theta_freqs / 2.0\n",
        "picks_n200 = [ch for ch in ['FZ', 'CZ'] if ch in channels]\n",
        "picks_p300 = [ch for ch in ['PZ', 'P3', 'P4'] if ch in channels]\n",
        "picks_theta = [ch for ch in ['FZ', 'CZ'] if ch in channels]\n",
        "\n",
        "# --- N200 Analysis based on Model Predictions ---\n",
        "print(\"\\n--- N200 Analysis (Based on Model Predictions) ---\")\n",
        "if picks_n200 and evoked_fast_pred is not None and evoked_slow_pred is not None:\n",
        "    fig_n200_pred = mne.viz.plot_compare_evokeds(\n",
        "        {'Fast RT (label 0)': evoked_fast_pred, 'Slow RT (label 1)': evoked_slow_pred},\n",
        "        picks=picks_n200,\n",
        "        title=f'N200 Comparison {\", \".join(picks_n200)})',\n",
        "        show_sensors='upper right',\n",
        "        legend='upper left',\n",
        "        ci=0.95\n",
        "    )\n",
        "    current_fig_n200_pred = fig_n200_pred[0] if isinstance(fig_n200_pred, list) else fig_n200_pred\n",
        "    for i, ax in enumerate(current_fig_n200_pred.axes[:len(picks_n200)]):\n",
        "        ax.axvspan(n200_window[0], n200_window[1], color='gray', alpha=0.2, label='N200 Window')\n",
        "        # Add vertical lines for potential peaks (you might need to calculate these as before)\n",
        "    plt.show()\n",
        "elif not picks_n200:\n",
        "    print(\"Skipping N200 analysis: No suitable channels found.\")\n",
        "else:\n",
        "    print(\"Skipping N200 analysis: Not enough predicted epochs for both conditions.\")\n",
        "\n",
        "# --- P300 Analysis based on Model Predictions ---\n",
        "print(\"\\n--- P300 Analysis (Based on Model Predictions) ---\")\n",
        "if picks_p300 and evoked_fast_pred is not None and evoked_slow_pred is not None:\n",
        "    fig_p300_pred = mne.viz.plot_compare_evokeds(\n",
        "        {'Fast RT (label 0)': evoked_fast_pred, 'Slow RT (label 1)': evoked_slow_pred},\n",
        "        picks=picks_p300,\n",
        "        title=f'P300 Comparison {\", \".join(picks_p300)})',\n",
        "        show_sensors='upper right',\n",
        "        legend='upper left',\n",
        "        ci=0.95\n",
        "    )\n",
        "    current_fig_p300_pred = fig_p300_pred[0] if isinstance(fig_p300_pred, list) else fig_p300_pred\n",
        "    for i, ax in enumerate(current_fig_p300_pred.axes[:len(picks_p300)]):\n",
        "        ax.axvspan(p300_window[0], p300_window[1], color='gray', alpha=0.2, label='P300 Window')\n",
        "        # Add vertical lines for potential peaks\n",
        "    plt.show()\n",
        "elif not picks_p300:\n",
        "    print(\"Skipping P300 analysis: No suitable channels found.\")\n",
        "else:\n",
        "    print(\"Skipping P300 analysis: Not enough predicted epochs for both conditions.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"picks_theta: {picks_theta}\")\n",
        "print(f\"Number of epochs predicted as fast RT (label 0): {len(epochs_fast_pred)}\")\n",
        "print(f\"Number of epochs predicted as slow RT (label 1): {len(epochs_slow_pred)}\")\n",
        "print(\"\\n--- Time-Frequency Analysis for Theta (Based on Model Predictions) ---\")\n",
        "\n",
        "# Ensure n_cycles_theta is compatible with epoch length and freqs\n",
        "# Example: n_cycles_theta = theta_freqs / 2.0\n",
        "\n",
        "if picks_theta and len(epochs_fast_pred) > 0 and len(epochs_slow_pred) > 0:\n",
        "    try:\n",
        "        # Use the newer .compute_tfr method\n",
        "        tfr_fast_pred = epochs_fast_pred.compute_tfr(\n",
        "            method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "            return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "        )\n",
        "        tfr_slow_pred = epochs_slow_pred.compute_tfr(\n",
        "            method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "            return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "        )\n",
        "\n",
        "        # Apply baseline correction (percentage change is common for power)\n",
        "        tfr_fast_pred.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "        tfr_slow_pred.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "\n",
        "        # Calculate the averaged TFR for plotting TFR maps\n",
        "        avg_tfr_fast_pred = tfr_fast_pred.average()\n",
        "        avg_tfr_slow_pred = tfr_slow_pred.average()\n",
        "\n",
        "\n",
        "        # --- Visualize Average Theta Power Over Time ---\n",
        "        times_tfr_pred = avg_tfr_fast_pred.times  # Get time points from the averaged TFR\n",
        "        theta_power_fast_pred = avg_tfr_fast_pred.data.mean(axis=(0, 1))  # Mean over channels & freqs\n",
        "        theta_power_slow_pred = avg_tfr_slow_pred.data.mean(axis=(0, 1))\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times_tfr_pred, theta_power_fast_pred, label='Fast Condition')\n",
        "        plt.plot(times_tfr_pred, theta_power_slow_pred, label='Slow Condition')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('Average Theta Power (% Change from Baseline)')\n",
        "        plt.title('Average Theta Power Over Time')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during TFR analysis (predicted labels): {e}\")\n",
        "elif not picks_theta:\n",
        "    print(\"Skipping Theta TFR analysis: No suitable channels found.\")\n",
        "else:\n",
        "    print(\"Skipping Theta TFR analysis: Not enough predicted epochs for both conditions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6ctaadeyoVs"
      },
      "outputs": [],
      "source": [
        "import mne.stats # Import MNE stats module\n",
        "\n",
        "# -\n",
        "if 'epochs' not in locals():\n",
        "    raise NameError(\"Original 'epochs' object not found. Please ensure your preceding code creates it.\")\n",
        "info = epochs.info\n",
        "\n",
        "# Ensure TMIN is defined for EpochsArray and baseline\n",
        "try:\n",
        "    TMIN # Check if TMIN is defined\n",
        "except NameError:\n",
        "     print(\"TMIN not found, using -0.2 as a default. Please define TMIN based on your epoching.\")\n",
        "     TMIN = -0.2 # Default value, replace with your actual TMIN\n",
        "\n",
        "baseline_period = (TMIN, 0) # Define baseline period\n",
        "\n",
        "\n",
        "# --- Create EpochsArray from Split Train Data (for True Label Analysis) ---\n",
        "print(\"\\n--- Preparing Train Data for Statistical Analysis (True Labels) ---\")\n",
        "\n",
        "if X_train_full.shape[-1] == len(info['ch_names']) and X_train_full.shape[1] != len(info['ch_names']):\n",
        "    print(f\"Transposing X_train_full from {X_train_full.shape} to (n_epochs, n_channels, n_times)\")\n",
        "    X_train_mne_shape = np.transpose(X_train_full, (0, 2, 1))\n",
        "else:\n",
        "     print(f\"X_train_full shape {X_train_full.shape} looks suitable for MNE (n_epochs, n_channels, n_times).\")\n",
        "     X_train_mne_shape = X_train_full # Assume it's already in the right shape\n",
        "\n",
        "\n",
        "# Create dummy events for EpochsArray (needed even if times are relative to epoch start)\n",
        "train_events = np.zeros((len(X_train_mne_shape), 3), dtype=int)\n",
        "train_events[:, 0] = np.arange(len(X_train_mne_shape)) # Dummy sample numbers starting from 0\n",
        "train_events[:, 2] = 1 # Using a dummy event ID\n",
        "\n",
        "# Create EpochsArray for Training Data using TRUE labels (y_train_full)\n",
        "epochs_train_array = mne.EpochsArray(\n",
        "    X_train_mne_shape,\n",
        "    info, # Use info from original epochs to get channel names and locations\n",
        "    events=train_events,\n",
        "    tmin=TMIN, # Use the correct epoch start time\n",
        "    verbose=False\n",
        ")\n",
        "# Add true training labels as metadata\n",
        "if len(y_train_full) != len(epochs_train_array):\n",
        "     raise ValueError(f\"Length of y_train_full ({len(y_train_full)}) does not match the number of train epochs ({len(epochs_train_array)}).\")\n",
        "epochs_train_array.metadata = pd.DataFrame({'label': y_train_full.flatten()})\n",
        "print(f\"Created epochs_train_array with {len(epochs_train_array)} epochs.\")\n",
        "\n",
        "# Filter based on the 'label' metadata (true labels)\n",
        "epochs_train_fast_true = epochs_train_array[epochs_train_array.metadata['label'] == 0]\n",
        "epochs_train_slow_true = epochs_train_array[epochs_train_array.metadata['label'] == 1]\n",
        "\n",
        "print(f\"Number of train epochs (True Labels) Fast RT (label 0): {len(epochs_train_fast_true)}\")\n",
        "print(f\"Number of train epochs (True Labels) Slow RT (label 1): {len(epochs_train_slow_true)}\")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    theta_freqs # Check if theta_freqs is defined\n",
        "except NameError:\n",
        "    print(\"theta_freqs not found. Skipping Theta TFR calculation for train data.\")\n",
        "    theta_freqs = None # Ensure it's None if not defined (overwrites check above for simplicity)\n",
        "\n",
        "tfr_fast_train_true = None\n",
        "tfr_slow_train_true = None\n",
        "\n",
        "# Ensure theta_freqs is defined for TFR\n",
        "if theta_freqs is not None and len(epochs_train_fast_true) > 0 and len(epochs_train_slow_true) > 0:\n",
        "    print(\"\\n--- Computing Time-Frequency (True Labels on Train Data) ---\")\n",
        "    # Ensure n_cycles_theta is compatible\n",
        "    n_cycles_theta = theta_freqs / 2.0 # Common choice\n",
        "\n",
        "    # Define picks for theta analysis (using all EEG channels if not specified)\n",
        "    try:\n",
        "        picks_theta # Check if picks_theta is defined\n",
        "    except NameError:\n",
        "         print(\"picks_theta not found. Defining default for Theta TFR (all EEG channels).\")\n",
        "         picks_theta = mne.pick_types(info, eeg=True, exclude='bads')\n",
        "         if len(picks_theta) == 0:\n",
        "             print(\"No EEG channels found after excluding bads, trying all channels.\")\n",
        "             picks_theta = mne.pick_types(info, eeg=True) # Try all if excluding bads results in none\n",
        "\n",
        "\n",
        "    if picks_theta and len(picks_theta) > 0:\n",
        "        try:\n",
        "            # Use the newer .compute_tfr method\n",
        "            tfr_fast_train_true = epochs_train_fast_true.compute_tfr(\n",
        "                method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "                return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "            )\n",
        "            tfr_slow_train_true = epochs_train_slow_true.compute_tfr(\n",
        "                method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "                return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "            )\n",
        "\n",
        "            # Apply baseline correction (percentage change is common for power)\n",
        "            tfr_fast_train_true.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "            tfr_slow_train_true.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "            print(\"Train TFR computation complete.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Train TFR computation: {e}\")\n",
        "            tfr_fast_train_true = None\n",
        "            tfr_slow_train_true = None\n",
        "    else:\n",
        "         print(\"Skipping Train TFR computation: No suitable channels found for picks_theta.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Train TFR computation: Not enough true train epochs or theta_freqs not defined.\")\n",
        "\n",
        "\n",
        "# --- Statistical Verification (True Labels on Train Data) ---\n",
        "print(\"\\n--- Statistical Verification (True Labels on Train Data) ---\")\n",
        "\n",
        "# --- 1. Statistical Test for Evoked Potential Differences (True Slow vs Fast on Train Data) ---\n",
        "print(\"\\n--> Comparing Evoked Potentials (True Slow vs Fast on Train Data)\")\n",
        "\n",
        "# Check if we have enough epochs in both true train conditions for stats\n",
        "if len(epochs_train_fast_true) > 1 and len(epochs_train_slow_true) > 1:\n",
        "    # Extract data arrays for the statistical test\n",
        "    # Shape should be (n_observations, n_channels, n_times)\n",
        "    data_fast_train = epochs_train_fast_true.get_data(copy=False).astype(np.float64)\n",
        "    data_slow_train = epochs_train_slow_true.get_data(copy=False).astype(np.float64)\n",
        "\n",
        "    # Define connectivity for the cluster test (based on channel layout)\n",
        "    # This assumes your `info` object has a montage with channel locations\n",
        "    try:\n",
        "        # Attempt to find connectivity. If it fails, the test will run without spatial connectivity.\n",
        "        connectivity, ch_names_conn = mne.channels.find_nearest_neighbors(info, ch_type='eeg')\n",
        "    except Exception:\n",
        "         try:\n",
        "             print(\"find_nearest_neighbors failed, trying find_layout...\")\n",
        "             connectivity, ch_names_conn = mne.channels.find_layout(info, ch_types='eeg').get_connectivity(info)\n",
        "         except Exception as e_conn:\n",
        "             print(f\"Could not determine channel connectivity: {e_conn}\")\n",
        "             print(\"Train Evoked cluster test will run without spatial connectivity.\")\n",
        "             connectivity = None\n",
        "\n",
        "\n",
        "    # --- Cluster-based permutation test ---\n",
        "    # This test compares two independent groups of epochs across time and channels\n",
        "    print(f\"Running spatio-temporal cluster permutation test ({'with' if connectivity is not None else 'without'} spatial connectivity)...\")\n",
        "    try:\n",
        "        # Set a threshold based on t-value. A value around 2.0 is a common starting point.\n",
        "        # This threshold determines which data points are included in potential clusters.\n",
        "        threshold = 2.0 # T-value threshold\n",
        "\n",
        "        # Number of permutations. More permutations give more accurate p-values but take longer.\n",
        "        # For publication, 10000+ is often used, but 1000-2000 is good for exploration.\n",
        "        n_permutations = 1000\n",
        "\n",
        "        T_obs_train, clusters_train, p_values_train, _ = mne.stats.spatio_temporal_cluster_test(\n",
        "            [data_slow_train, data_fast_train], # List of data arrays for each group/condition (Slow vs Fast)\n",
        "            n_permutations=n_permutations,\n",
        "            threshold=threshold,\n",
        "            tail=0, # 0: two-tailed (look for slow > fast or slow < fast)\n",
        "            stat_fun='indep_t', # Independent t-test suitable for comparing two groups of epochs\n",
        "            connectivity=connectivity, # Use determined connectivity (or None)\n",
        "            n_jobs=-1, # Use all available cores\n",
        "            verbose=False # Set to True for more details during the test\n",
        "        )\n",
        "\n",
        "        print(\"Train Evoked cluster permutation test complete.\")\n",
        "\n",
        "        # --- Report significant clusters ---\n",
        "        alpha = 0.05 # Significance level for the clusters\n",
        "        good_clusters_train_idx = [i for i, p_val in enumerate(p_values_train) if p_val < alpha]\n",
        "\n",
        "        if good_clusters_train_idx:\n",
        "            print(f\"\\nFound {len(good_clusters_train_idx)} significant spatio-temporal clusters for Train Evoked (p < {alpha}):\")\n",
        "            # Get the time vector from the epochs object\n",
        "            times = epochs_train_array.times\n",
        "            for i in good_clusters_train_idx:\n",
        "                T_obs_this_cluster = T_obs_train[clusters_train[i]] # Get the T-values within this cluster\n",
        "                min_t = np.min(T_obs_this_cluster)\n",
        "                max_t = np.max(T_obs_this_cluster)\n",
        "                cluster_p_value = p_values_train[i]\n",
        "\n",
        "                # Get cluster spatial and temporal indices\n",
        "                cluster_channels_idx, cluster_times_idx = clusters_train[i]\n",
        "\n",
        "                # Convert time indices to seconds and find range\n",
        "                unique_times_idx = np.unique(cluster_times_idx)\n",
        "                cluster_times = times[unique_times_idx]\n",
        "                tmin_cluster, tmax_cluster = cluster_times.min(), cluster_times.max()\n",
        "\n",
        "                # Get channel names involved in the cluster\n",
        "                unique_channels_idx = np.unique(cluster_channels_idx)\n",
        "                cluster_channel_names = [epochs_train_array.ch_names[j] for j in unique_channels_idx]\n",
        "\n",
        "                print(f\"  - Train Cluster {i+1}: p-value = {cluster_p_value:.4f}\")\n",
        "                print(f\"    Time range: [{tmin_cluster:.3f} s, {tmax_cluster:.3f} s]\")\n",
        "                print(f\"    Channels ({len(unique_channels_idx)}) : {cluster_channel_names}\")\n",
        "                print(f\"    T-value range: [{min_t:.2f}, {max_t:.2f}]\")\n",
        "                # Optional: You could add code here to visualize this cluster on the evoked average plot for the train data\n",
        "\n",
        "        else:\n",
        "            print(f\"\\nNo significant spatio-temporal clusters found for Train Evoked at p < {alpha}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during the Train Evoked cluster test: {e}\")\n",
        "        print(\"Skipping Train Evoked cluster test due to error.\")\n",
        "else:\n",
        "    print(\"\\nSkipping Train Evoked cluster test: Not enough epochs (min 2 per condition) in true train sets.\")\n",
        "\n",
        "\n",
        "# --- 2. Statistical Test for Time-Frequency Power Differences (True Slow vs Fast on Train Data) ---\n",
        "print(\"\\n--> Comparing Theta Power (True Slow vs Fast on Train Data)\")\n",
        "\n",
        "# Ensure TFR objects for true train data are available and have enough epochs\n",
        "if tfr_fast_train_true is not None and tfr_slow_train_true is not None and len(tfr_fast_train_true.data) > 1 and len(tfr_slow_train_true.data) > 1:\n",
        "    # Extract power data\n",
        "    # Shape is (n_epochs, n_channels, n_freqs, n_times)\n",
        "    power_fast_train = tfr_fast_train_true.data.astype(np.float64)\n",
        "    power_slow_train = tfr_slow_train_true.data.astype(np.float64)\n",
        "\n",
        "    # Define connectivity (using the same spatial connectivity from evoked)\n",
        "    # Find frequency indices for the theta band\n",
        "    try:\n",
        "        freqs = tfr_fast_train_true.freqs # Get the actual frequencies from the TFR object\n",
        "        if theta_freqs is None:\n",
        "             print(\"theta_freqs not defined, cannot find indices for theta band.\")\n",
        "             theta_freq_indices = []\n",
        "        else:\n",
        "             # Ensure theta_freqs match the freqs in the TFR object calculation\n",
        "             theta_freq_indices = np.where(np.logical_and(freqs >= theta_freqs.min(), freqs <= theta_freqs.max()))[0]\n",
        "             if len(theta_freq_indices) == 0:\n",
        "                 print(f\"Warning: No frequencies in TFR object fall within the specified theta band ({theta_freqs.min()}-{theta_freqs.max()} Hz).\")\n",
        "\n",
        "    except Exception as e_freq:\n",
        "         print(f\"Could not get frequencies from Train TFR object or find theta band indices: {e_freq}\")\n",
        "         theta_freq_indices = []\n",
        "\n",
        "\n",
        "    if len(theta_freq_indices) > 0:\n",
        "         print(f\"Averaging power over {len(theta_freq_indices)} frequencies in the theta band for Train TFR cluster test.\")\n",
        "         # Ensure we average over a valid axis\n",
        "         if power_fast_train.ndim == 4 and power_fast_train.shape[2] > 0:\n",
        "             power_fast_train_theta_avg = power_fast_train[:, :, theta_freq_indices, :].mean(axis=2) # Shape (n_epochs, n_channels, n_times)\n",
        "             power_slow_train_theta_avg = power_slow_train[:, :, theta_freq_indices, :].mean(axis=2) # Shape (n_epochs, n_channels, n_times)\n",
        "         else:\n",
        "              print(\"Train TFR power data shape is not as expected for averaging over frequency (expected 4D).\")\n",
        "              power_fast_train_theta_avg = None\n",
        "              power_slow_train_theta_avg = None\n",
        "    else:\n",
        "         print(\"Skipping Train TFR cluster test: No frequencies found in the specified theta band or issues getting freq indices.\")\n",
        "         power_fast_train_theta_avg = None\n",
        "         power_slow_train_theta_avg = None\n",
        "\n",
        "\n",
        "    if power_fast_train_theta_avg is not None and power_slow_train_theta_avg is not None:\n",
        "         print(f\"Running spatio-temporal cluster permutation test for average Train Theta power ({'with' if connectivity is not None else 'without'} spatial connectivity)...\")\n",
        "\n",
        "         try:\n",
        "             # Use similar threshold and permutations\n",
        "             threshold_tf = 2.0\n",
        "             n_permutations_tf = 1000\n",
        "\n",
        "             T_obs_tf_train, clusters_tf_train, p_values_tf_train, _ = mne.stats.spatio_temporal_cluster_test(\n",
        "                 [power_slow_train_theta_avg, power_fast_train_theta_avg], # Slow vs Fast\n",
        "                 n_permutations=n_permutations_tf,\n",
        "                 threshold=threshold_tf,\n",
        "                 tail=0, # Two-tailed\n",
        "                 stat_fun='indep_t',\n",
        "                 connectivity=connectivity, # Spatial connectivity (reused from evoked test)\n",
        "                 n_jobs=-1,\n",
        "                 verbose=False\n",
        "             )\n",
        "\n",
        "             print(\"Train Theta power cluster permutation test complete.\")\n",
        "\n",
        "             # --- Report significant clusters ---\n",
        "             alpha = 0.05\n",
        "             good_clusters_tf_train_idx = [i for i, p_val in enumerate(p_values_tf_train) if p_val < alpha]\n",
        "\n",
        "             if good_clusters_tf_train_idx:\n",
        "                 print(f\"\\nFound {len(good_clusters_tf_train_idx)} significant spatio-temporal clusters for Train Theta power (p < {alpha}):\")\n",
        "                 times_tf = tfr_fast_train_true.times # Or epochs_train_array.times\n",
        "                 for i in good_clusters_tf_train_idx:\n",
        "                     T_obs_this_cluster = T_obs_tf_train[clusters_tf_train[i]]\n",
        "                     min_t = np.min(T_obs_this_cluster)\n",
        "                     max_t = np.max(T_obs_this_cluster)\n",
        "                     cluster_p_value = p_values_tf_train[i]\n",
        "\n",
        "                     cluster_channels_idx, cluster_times_idx = clusters_tf_train[i]\n",
        "                     unique_times_idx = np.unique(cluster_times_idx)\n",
        "                     cluster_times = times_tf[unique_times_idx]\n",
        "                     tmin_cluster, tmax_cluster = cluster_times.min(), cluster_times.max()\n",
        "\n",
        "                     unique_channels_idx = np.unique(cluster_channels_idx)\n",
        "                     cluster_channel_names = [tfr_fast_train_true.ch_names[j] for j in unique_channels_idx] # Or epochs_train_array.ch_names\n",
        "\n",
        "                     print(f\"  - Train Theta Cluster {i+1}: p-value = {cluster_p_value:.4f}\")\n",
        "                     print(f\"    Time range: [{tmin_cluster:.3f} s, {tmax_cluster:.3f} s]\")\n",
        "                     print(f\"    Channels ({len(unique_channels_idx)}) : {cluster_channel_names}\")\n",
        "                     print(f\"    T-value range: [{min_t:.2f}, {max_t:.2f}]\")\n",
        "                     # Optional: Add plotting code here to visualize this cluster on the TFR plot for the train data\n",
        "\n",
        "             else:\n",
        "                 print(f\"\\nNo significant spatio-temporal clusters found for Train Theta power at p < {alpha}.\")\n",
        "\n",
        "         except Exception as e:\n",
        "             print(f\"An unexpected error occurred during the Train Theta power cluster test: {e}\")\n",
        "             print(\"Skipping Train Theta power cluster test due to error.\")\n",
        "    else:\n",
        "         print(\"\\nSkipping Train TFR cluster test: Data averaging failed or data not available.\")\n",
        "\n",
        "else:\n",
        "     print(\"\\nSkipping Train TFR cluster test: Not enough epochs (min 2 per condition) or TFR objects not available for true train sets.\")\n",
        "\n",
        "\n",
        "# --- Create EpochsArray from Split Test Data (for Prediction Analysis) ---\n",
        "# This section prepares the data for the Test (Predicted) stats\n",
        "\n",
        "print(\"\\n--- Preparing Test Data for Statistical Analysis (Predicted Labels) ---\")\n",
        "\n",
        "# MNE's EpochsArray expects data in shape (n_epochs, n_channels, n_times)\n",
        "# Check the shape of X_test_full and transpose if needed\n",
        "# Assuming X_test_full is (n_test_epochs, n_times, n_channels)\n",
        "if X_test_full.shape[-1] == len(info['ch_names']) and X_test_full.shape[1] != len(info['ch_names']):\n",
        "    print(f\"Transposing X_test_full from {X_test_full.shape} to (n_epochs, n_channels, n_times)\")\n",
        "    X_test_mne_shape = np.transpose(X_test_full, (0, 2, 1))\n",
        "else:\n",
        "    print(f\"X_test_full shape {X_test_full.shape} looks suitable for MNE (n_epochs, n_channels, n_times).\")\n",
        "    X_test_mne_shape = X_test_full # Assume it's already in the right shape\n",
        "\n",
        "\n",
        "# Create dummy events for EpochsArray\n",
        "test_events = np.zeros((len(X_test_mne_shape), 3), dtype=int)\n",
        "test_events[:, 0] = np.arange(len(X_test_mne_shape))\n",
        "test_events[:, 2] = 1 # Dummy event ID\n",
        "\n",
        "# Create EpochsArray for Test Data using PREDICTED labels\n",
        "epochs_test_pred_array = mne.EpochsArray(\n",
        "    X_test_mne_shape,\n",
        "    info,\n",
        "    events=test_events,\n",
        "    tmin=TMIN,\n",
        "    verbose=False\n",
        ")\n",
        "# Add predicted test labels as metadata\n",
        "if len(y_pred) != len(epochs_test_pred_array):\n",
        "     raise ValueError(f\"Length of y_pred ({len(y_pred)}) does not match the number of test epochs ({len(epochs_test_pred_array)}).\")\n",
        "epochs_test_pred_array.metadata = pd.DataFrame({'predicted_label': y_pred.flatten()})\n",
        "print(f\"Created epochs_test_pred_array with {len(epochs_test_pred_array)} epochs.\")\n",
        "\n",
        "# Filter based on the 'predicted_label' metadata\n",
        "epochs_test_fast_pred = epochs_test_pred_array[epochs_test_pred_array.metadata['predicted_label'] == 0]\n",
        "epochs_test_slow_pred = epochs_test_pred_array[epochs_test_pred_array.metadata['predicted_label'] == 1]\n",
        "\n",
        "print(f\"Number of test epochs predicted as fast RT (label 0): {len(epochs_test_fast_pred)}\")\n",
        "print(f\"Number of test epochs predicted as slow RT (label 1): {len(epochs_test_slow_pred)}\")\n",
        "\n",
        "\n",
        "# --- Compute Time-Frequency (for Predicted Labels on Test Data) ---\n",
        "# This is needed for Test (Predicted) stats\n",
        "\n",
        "# Ensure theta_freqs is defined for TFR\n",
        "if theta_freqs is not None and len(epochs_test_fast_pred) > 0 and len(epochs_test_slow_pred) > 0:\n",
        "    print(\"\\n--- Computing Time-Frequency (Predicted Labels on Test Data) ---\")\n",
        "    # Ensure n_cycles_theta is compatible\n",
        "    n_cycles_theta = theta_freqs / 2.0 # Common choice\n",
        "\n",
        "    # Define picks for theta analysis (using all EEG channels if not specified)\n",
        "    try:\n",
        "        picks_theta # Check if picks_theta is defined (defined earlier for train TFR)\n",
        "    except NameError:\n",
        "         print(\"picks_theta not found. Defining default for Theta TFR (all EEG channels).\")\n",
        "         picks_theta = mne.pick_types(info, eeg=True, exclude='bads')\n",
        "         if len(picks_theta) == 0:\n",
        "             print(\"No EEG channels found after excluding bads, trying all channels.\")\n",
        "             picks_theta = mne.pick_types(info, eeg=True)\n",
        "\n",
        "    if picks_theta and len(picks_theta) > 0:\n",
        "        try:\n",
        "            tfr_fast_pred = epochs_test_fast_pred.compute_tfr(\n",
        "                method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "                return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "            )\n",
        "            tfr_slow_pred = epochs_test_slow_pred.compute_tfr(\n",
        "                method=\"morlet\", freqs=theta_freqs, n_cycles=n_cycles_theta, use_fft=True,\n",
        "                return_itc=False, decim=3, n_jobs=-1, picks=picks_theta, verbose=False\n",
        "            )\n",
        "\n",
        "            # Apply baseline correction\n",
        "            tfr_fast_pred.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "            tfr_slow_pred.apply_baseline(baseline=baseline_period, mode='percent')\n",
        "            print(\"Test TFR computation complete.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Test TFR computation for predicted labels: {e}\")\n",
        "            tfr_fast_pred = None\n",
        "            tfr_slow_pred = None\n",
        "    else:\n",
        "         print(\"Skipping Test TFR computation: No suitable channels found for picks_theta.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Test TFR computation: Not enough predicted test epochs or theta_freqs not defined.\")\n",
        "\n",
        "\n",
        "# --- Statistical Verification (Predicted Labels on Test Data) ---\n",
        "# Adding Cluster-based permutation tests\n",
        "\n",
        "print(\"\\n--- Statistical Verification (Predicted Labels on Test Data) ---\")\n",
        "\n",
        "# --- 1. Statistical Test for Evoked Potential Differences (Predicted Slow vs Fast on Test Data) ---\n",
        "print(\"\\n--> Comparing Evoked Potentials (Predicted Slow vs Fast on Test Data)\")\n",
        "\n",
        "# Check if we have enough epochs in both predicted conditions on the test set for stats\n",
        "if len(epochs_test_fast_pred) > 1 and len(epochs_test_slow_pred) > 1:\n",
        "    # Extract data arrays for the statistical test\n",
        "    data_fast = epochs_test_fast_pred.get_data(copy=False).astype(np.float64)\n",
        "    data_slow = epochs_test_slow_pred.get_data(copy=False).astype(np.float64)\n",
        "\n",
        "    # Define connectivity (reuse the connectivity found during train stats if successful)\n",
        "    try:\n",
        "        connectivity # Check if connectivity was found during train stats\n",
        "    except NameError:\n",
        "         print(\"Connectivity not found from train stats, trying to find it now for test stats.\")\n",
        "         try:\n",
        "            connectivity, ch_names_conn = mne.channels.find_nearest_neighbors(info, ch_type='eeg')\n",
        "         except Exception:\n",
        "             try:\n",
        "                 print(\"find_nearest_neighbors failed, trying find_layout...\")\n",
        "                 connectivity, ch_names_conn = mne.channels.find_layout(info, ch_types='eeg').get_connectivity(info)\n",
        "             except Exception as e_conn:\n",
        "                 print(f\"Could not determine channel connectivity: {e_conn}\")\n",
        "                 print(\"Test Evoked cluster test will run without spatial connectivity.\")\n",
        "                 connectivity = None\n",
        "\n",
        "\n",
        "    # --- Cluster-based permutation test ---\n",
        "    print(f\"Running spatio-temporal cluster permutation test ({'with' if connectivity is not None else 'without'} spatial connectivity)...\")\n",
        "    try:\n",
        "        # Use the same threshold and permutations as for the train data for comparison\n",
        "        threshold = 2.0 # T-value threshold\n",
        "\n",
        "        # Number of permutations. More permutations give more accurate p-values but take longer.\n",
        "        n_permutations = 1000\n",
        "\n",
        "        T_obs_test, clusters_test, p_values_test, _ = mne.stats.spatio_temporal_cluster_test(\n",
        "            [data_slow, data_fast], # List of data arrays for each group/condition (Slow vs Fast)\n",
        "            n_permutations=n_permutations,\n",
        "            threshold=threshold,\n",
        "            tail=0, # Two-tailed\n",
        "            stat_fun='indep_t',\n",
        "            connectivity=connectivity, # Use determined connectivity (or None)\n",
        "            n_jobs=-1, # Use all available cores\n",
        "            verbose=False # Set to True for more details during the test\n",
        "        )\n",
        "\n",
        "        print(\"Test Evoked cluster permutation test complete.\")\n",
        "\n",
        "        # --- Report significant clusters ---\n",
        "        alpha = 0.05 # Significance level for the clusters\n",
        "        good_clusters_test_idx = [i for i, p_val in enumerate(p_values_test) if p_val < alpha]\n",
        "\n",
        "        if good_clusters_test_idx:\n",
        "            print(f\"\\nFound {len(good_clusters_test_idx)} significant spatio-temporal clusters for Test Evoked (p < {alpha}):\")\n",
        "            # Get the time vector from the epochs object\n",
        "            times = epochs_test_pred_array.times\n",
        "            for i in good_clusters_test_idx:\n",
        "                T_obs_this_cluster = T_obs_test[clusters_test[i]] # Get the T-values within this cluster\n",
        "                min_t = np.min(T_obs_this_cluster)\n",
        "                max_t = np.max(T_obs_this_cluster)\n",
        "                cluster_p_value = p_values_test[i]\n",
        "\n",
        "                # Get cluster spatial and temporal indices\n",
        "                cluster_channels_idx, cluster_times_idx = clusters_test[i]\n",
        "\n",
        "                # Convert time indices to seconds and find range\n",
        "                unique_times_idx = np.unique(cluster_times_idx)\n",
        "                cluster_times = times[unique_times_idx]\n",
        "                tmin_cluster, tmax_cluster = cluster_times.min(), cluster_times.max()\n",
        "\n",
        "                # Get channel names involved in the cluster\n",
        "                unique_channels_idx = np.unique(cluster_channels_idx)\n",
        "                cluster_channel_names = [epochs_test_pred_array.ch_names[j] for j in unique_channels_idx]\n",
        "\n",
        "                print(f\"  - Test Cluster {i+1}: p-value = {cluster_p_value:.4f}\")\n",
        "                print(f\"    Time range: [{tmin_cluster:.3f} s, {tmax_cluster:.3f} s]\")\n",
        "                print(f\"    Channels ({len(unique_channels_idx)}) : {cluster_channel_names}\")\n",
        "                print(f\"    T-value range: [{min_t:.2f}, {max_t:.2f}]\")\n",
        "                # Optional: You could add code here to visualize this cluster on the evoked average plot for the test data\n",
        "\n",
        "        else:\n",
        "            print(f\"\\nNo significant spatio-temporal clusters found for Test Evoked at p < {alpha}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during the Test Evoked cluster test: {e}\")\n",
        "        print(\"Skipping Test Evoked cluster test due to error.\")\n",
        "else:\n",
        "    print(\"\\nSkipping Test Evoked cluster test: Not enough epochs (min 2 per condition) in predicted test sets.\")\n",
        "\n",
        "\n",
        "# --- 2. Statistical Test for Time-Frequency Power Differences (Predicted Slow vs Fast on Test Data) ---\n",
        "print(\"\\n--> Comparing Theta Power (Predicted Slow vs Fast on Test Data)\")\n",
        "\n",
        "# Ensure TFR objects for test data predicted labels are available and have enough epochs\n",
        "if tfr_fast_pred is not None and tfr_slow_pred is not None and len(tfr_fast_pred.data) > 1 and len(tfr_slow_pred.data) > 1:\n",
        "    # Extract power data\n",
        "    power_fast = tfr_fast_pred.data.astype(np.float64)\n",
        "    power_slow = tfr_slow_pred.data.astype(np.float64)\n",
        "\n",
        "    # Define connectivity (using the same spatial connectivity)\n",
        "    # Find frequency indices for the theta band\n",
        "    try:\n",
        "        freqs = tfr_fast_pred.freqs\n",
        "        if theta_freqs is None:\n",
        "             print(\"theta_freqs not defined, cannot find indices for theta band.\")\n",
        "             theta_freq_indices = []\n",
        "        else:\n",
        "             # Ensure theta_freqs match the freqs in the TFR object calculation\n",
        "             theta_freq_indices = np.where(np.logical_and(freqs >= theta_freqs.min(), freqs <= theta_freqs.max()))[0]\n",
        "             if len(theta_freq_indices) == 0:\n",
        "                 print(f\"Warning: No frequencies in TFR object fall within the specified theta band ({theta_freqs.min()}-{theta_freqs.max()} Hz).\")\n",
        "\n",
        "    except Exception as e_freq:\n",
        "         print(f\"Could not get frequencies from Test TFR object or find theta band indices: {e_freq}\")\n",
        "         theta_freq_indices = []\n",
        "\n",
        "\n",
        "    if len(theta_freq_indices) > 0:\n",
        "         print(f\"Averaging power over {len(theta_freq_indices)} frequencies in the theta band for Test TFR cluster test.\")\n",
        "         # Ensure we average over a valid axis\n",
        "         if power_fast.ndim == 4 and power_fast.shape[2] > 0:\n",
        "             power_fast_theta_avg = power_fast[:, :, theta_freq_indices, :].mean(axis=2) # Shape (n_epochs, n_channels, n_times)\n",
        "             power_slow_theta_avg = power_slow[:, :, theta_freq_indices, :].mean(axis=2) # Shape (n_epochs, n_channels, n_times)\n",
        "         else:\n",
        "              print(\"Test TFR power data shape is not as expected for averaging over frequency (expected 4D).\")\n",
        "              power_fast_theta_avg = None\n",
        "              power_slow_theta_avg = None\n",
        "    else:\n",
        "         print(\"Skipping Test TFR cluster test: No frequencies found in the specified theta band or issues getting freq indices.\")\n",
        "         power_fast_theta_avg = None\n",
        "         power_slow_theta_avg = None\n",
        "\n",
        "\n",
        "    if power_fast_theta_avg is not None and power_slow_theta_avg is not None:\n",
        "         print(f\"Running spatio-temporal cluster permutation test for average Test Theta power ({'with' if connectivity is not None else 'without'} spatial connectivity)...\")\n",
        "\n",
        "         try:\n",
        "             # Use similar threshold and permutations\n",
        "             threshold_tf = 2.0\n",
        "             n_permutations_tf = 1000\n",
        "\n",
        "             T_obs_tf_test, clusters_tf_test, p_values_tf_test, _ = mne.stats.spatio_temporal_cluster_test(\n",
        "                 [power_slow_theta_avg, power_fast_theta_avg], # Slow vs Fast\n",
        "                 n_permutations=n_permutations_tf,\n",
        "                 threshold=threshold_tf,\n",
        "                 tail=0, # Two-tailed\n",
        "                 stat_fun='indep_t',\n",
        "                 connectivity=connectivity, # Spatial connectivity (reused from evoked test)\n",
        "                 n_jobs=-1,\n",
        "                 verbose=False\n",
        "             )\n",
        "\n",
        "             print(\"Test Theta power cluster permutation test complete.\")\n",
        "\n",
        "             # --- Report significant clusters ---\n",
        "             alpha = 0.05\n",
        "             good_clusters_tf_test_idx = [i for i, p_val in enumerate(p_values_tf_test) if p_val < alpha]\n",
        "\n",
        "             if good_clusters_tf_test_idx:\n",
        "                 print(f\"\\nFound {len(good_clusters_tf_test_idx)} significant spatio-temporal clusters for Test Theta power (p < {alpha}):\")\n",
        "                 times_tf = tfr_fast_pred.times # Or epochs_test_pred_array.times\n",
        "                 for i in good_clusters_tf_test_idx:\n",
        "                     T_obs_this_cluster = T_obs_tf_test[clusters_tf_test[i]]\n",
        "                     min_t = np.min(T_obs_this_cluster)\n",
        "                     max_t = np.max(T_obs_this_cluster)\n",
        "                     cluster_p_value = p_values_tf_test[i]\n",
        "\n",
        "                     cluster_channels_idx, cluster_times_idx = clusters_tf_test[i]\n",
        "                     unique_times_idx = np.unique(cluster_times_idx)\n",
        "                     cluster_times = times_tf[unique_times_idx]\n",
        "                     tmin_cluster, tmax_cluster = cluster_times.min(), cluster_times.max()\n",
        "\n",
        "                     unique_channels_idx = np.unique(cluster_channels_idx)\n",
        "                     cluster_channel_names = [tfr_fast_pred.ch_names[j] for j in unique_channels_idx] # Or epochs_test_pred_array.ch_names\n",
        "\n",
        "                     print(f\"  - Test Theta Cluster {i+1}: p-value = {cluster_p_value:.4f}\")\n",
        "                     print(f\"    Time range: [{tmin_cluster:.3f} s, {tmax_cluster:.3f} s]\")\n",
        "                     print(f\"    Channels ({len(unique_channels_idx)}) : {cluster_channel_names}\")\n",
        "                     print(f\"    T-value range: [{min_t:.2f}, {max_t:.2f}]\")\n",
        "                     # Optional: You could add code here to visualize this cluster on the TFR plot for the test data\n",
        "\n",
        "             else:\n",
        "                 print(f\"\\nNo significant spatio-temporal clusters found for Test Theta power at p < {alpha}.\")\n",
        "\n",
        "         except Exception as e:\n",
        "             print(f\"An unexpected error occurred during the Test Theta power cluster test: {e}\")\n",
        "             print(\"Skipping Test Theta power cluster test due to error.\")\n",
        "    else:\n",
        "         print(\"\\nSkipping Test TFR cluster test: Data averaging failed or data not available.\")\n",
        "\n",
        "else:\n",
        "     print(\"\\nSkipping Test TFR cluster test: Not enough epochs (min 2 per condition) or TFR objects not available for predicted test sets.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- All Statistical Verification Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urs7EOWM0wA8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mne\n",
        "from mne.stats import spatio_temporal_cluster_test\n",
        "from mne.channels import find_ch_adjacency\n",
        "\n",
        "# 1) Compute the differenceâ€‘wave arrays\n",
        "data_train = epochs_fast_train.get_data() - epochs_slow_train.get_data()\n",
        "data_test  = epochs_fast_test.get_data()  - epochs_slow_test.get_data()\n",
        "\n",
        "# 2) Build adjacency for EEG sensors\n",
        "adjacency, _ = find_ch_adjacency(info, ch_type='eeg')\n",
        "\n",
        "# 3) Cluster permutation test (train vs. test differenceâ€‘waves)\n",
        "X = [data_train, data_test]  # list of two arrays: (n_epochs, n_channels, n_times)\n",
        "T_obs, clusters, cluster_p_values, H0 = spatio_temporal_cluster_test(\n",
        "    X,\n",
        "    n_permutations=1000,\n",
        "    adjacency=adjacency,\n",
        "    tail=0,            # twoâ€‘tailed test\n",
        "    n_jobs=1,\n",
        "    out_type='mask'    # clusters as boolean masks\n",
        ")\n",
        "\n",
        "# 4) Report significant clusters\n",
        "sig_idxs = np.where(cluster_p_values < 0.05)[0]\n",
        "print(\"Significant cluster indices:\", sig_idxs)\n",
        "\n",
        "# 5) Plot the first significant cluster (if any)\n",
        "if sig_idxs.size > 0:\n",
        "    for clu in sig_idxs:\n",
        "        mask = clusters[clu]  # shape: (n_channels, n_times)\n",
        "        times = epochs_fast_train.times\n",
        "        # Find peak time within this cluster\n",
        "        t_inds = np.where(mask.any(axis=0))[0]\n",
        "        t_peak = times[t_inds[np.argmax(np.abs(T_obs[:, t_inds]).mean(axis=0))]]\n",
        "        # Topomap at peak\n",
        "        mne.viz.plot_topomap(\n",
        "            T_obs[:, times.tolist().index(t_peak)],\n",
        "            info,\n",
        "            mask=mask[:, times.tolist().index(t_peak)],\n",
        "            show=True,\n",
        "            title=f\"Cluster {clu} at {t_peak*1000:.0f}Â ms\"\n",
        "        )\n",
        "else:\n",
        "    print(\"No significant trainâ€“test cluster differences found (p < 0.05).\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPPqnBpxyUhByA5XW+Ahrrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}